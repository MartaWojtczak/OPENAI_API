{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###NOT FINISHED####\n",
    "###problems with reddit api - it is paid now and requires extra paperwork\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import praw\n",
    "from typing import List, Dict, Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\".env\")\n",
    "openai.api_key = os.environ[\"API\"]\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.environ[\"REDDIT_CLIENT\"],\n",
    "    client_secret=os.environ[\"REDDIT_SECRET\"],\n",
    "    user_agent=f\"script:test:0.0.1 (by u/purl_knittingpat)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_COLUMNS = [\"subreddit\", \"submission_id\", \"score\", \"comment_body\"]\n",
    "\n",
    "filename, subreddits = \"cities.csv\", [\n",
    "    \"NYC\",\n",
    "    \"Seattle\",\n",
    "    \"LosAngeles\",\n",
    "    \"Chicago\",\n",
    "    \"Austin\",\n",
    "    \"Portland\",\n",
    "    \"SanFrancisco\",\n",
    "    \"Boston\",\n",
    "    \"Houston\",\n",
    "    \"Atlanta\",\n",
    "    \"Philadelphia\",\n",
    "    \"Denver\",\n",
    "    \"SeattleWa\",\n",
    "    \"Dallas\",\n",
    "    \"WashingtonDC\",\n",
    "    \"SanDiego\",\n",
    "    \"Pittsburgh\",\n",
    "    \"Phoenix\",\n",
    "    \"Minneapolis\",\n",
    "    \"Orlando\",\n",
    "    \"Nashville\",\n",
    "    \"StLouis\",\n",
    "    \"SaltLakeCity\",\n",
    "    \"Columbus\",\n",
    "    \"Raleigh\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for fetching comments from submissions\n",
    "def comment_generator(submission) -> Generator:\n",
    "    # Do not bother expanding MoreComments (follow-links)\n",
    "    for comment in submission.comments.list():\n",
    "        if hasattr(comment, \"body\") and comment.body != \"[deleted]\" and comment.body != \"[removed]\":\n",
    "            yield (comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_comments(\n",
    "    filename: str,\n",
    "    target_comments_per_subreddit: int,\n",
    "    max_comments_per_submission: int,\n",
    "    max_comment_length: int,\n",
    "    reddit: praw.Reddit,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Collect comments from the top submissions in each subreddit.\n",
    "\n",
    "    Cache results at cache_filename.\n",
    "\n",
    "    Return a dataframe with columns: subreddit, submission_id, score, comment_body\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filename, index_col=\"id\")\n",
    "        assert df.columns.tolist() == DF_COLUMNS\n",
    "    except FileNotFoundError:\n",
    "        df = pd.DataFrame(columns=DF_COLUMNS)\n",
    "\n",
    "    # dict like {comment_id -> {column -> value}}\n",
    "    records = df.to_dict(orient=\"index\")\n",
    "\n",
    "    for subreddit_index, subreddit_name in enumerate(subreddits):\n",
    "        print(f\"Processing Subreddit: {subreddit_name}\")\n",
    "\n",
    "        processed_comments_for_subreddit = len(df[df[\"subreddit\"] == subreddit_name])\n",
    "\n",
    "        if processed_comments_for_subreddit >= target_comments_per_subreddit:\n",
    "            print(f\"Enough comments fetched for {subreddit_name}, continuing to next subreddit.\")\n",
    "            continue\n",
    "\n",
    "        # `top`` is a generator, grab submissions until we break (within this loop).\n",
    "        for submission in reddit.subreddit(subreddit_name).top(time_filter=\"month\"):\n",
    "            if processed_comments_for_subreddit >= target_comments_per_subreddit:\n",
    "                break\n",
    "\n",
    "            # The number of comments that we already have for this subreddit\n",
    "            processed_comments_for_submission = len(df[df[\"submission_id\"] == submission.id])\n",
    "\n",
    "            for comment in comment_generator(submission):\n",
    "                if processed_comments_for_submission >= max_comments_per_submission or processed_comments_for_subreddit >= target_comments_per_subreddit:\n",
    "                    break\n",
    "\n",
    "                if comment.id in records:\n",
    "                    print(f\"Skipping comment {subreddit_name}-{submission.id}-{comment.id} because we already have it\")\n",
    "                    continue\n",
    "\n",
    "                body = comment.body[:max_comment_length].strip()\n",
    "                records[comment.id] = {\"subreddit\": subreddit_name, \"submission_id\": submission.id, \"comment_body\": body}\n",
    "\n",
    "                processed_comments_for_subreddit += 1\n",
    "                processed_comments_for_submission += 1\n",
    "\n",
    "            # Once per post write to disk.\n",
    "            print(f\"CSV rewritten with {len(records)} rows.\\n\")\n",
    "            df = pd.DataFrame.from_dict(records, orient=\"index\", columns=DF_COLUMNS)\n",
    "            df.to_csv(filename, index_label=\"id\")\n",
    "\n",
    "    print(\"Completed.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Subreddit: NYC\n",
      "CSV rewritten with 10 rows.\n",
      "\n",
      "CSV rewritten with 20 rows.\n",
      "\n",
      "CSV rewritten with 30 rows.\n",
      "\n",
      "CSV rewritten with 40 rows.\n",
      "\n",
      "CSV rewritten with 50 rows.\n",
      "\n",
      "Processing Subreddit: Seattle\n",
      "CSV rewritten with 60 rows.\n",
      "\n",
      "CSV rewritten with 70 rows.\n",
      "\n",
      "CSV rewritten with 80 rows.\n",
      "\n",
      "CSV rewritten with 90 rows.\n",
      "\n",
      "CSV rewritten with 100 rows.\n",
      "\n",
      "Processing Subreddit: LosAngeles\n",
      "CSV rewritten with 110 rows.\n",
      "\n",
      "CSV rewritten with 120 rows.\n",
      "\n",
      "CSV rewritten with 130 rows.\n",
      "\n",
      "CSV rewritten with 140 rows.\n",
      "\n",
      "CSV rewritten with 150 rows.\n",
      "\n",
      "Processing Subreddit: Chicago\n",
      "CSV rewritten with 160 rows.\n",
      "\n",
      "CSV rewritten with 170 rows.\n",
      "\n",
      "CSV rewritten with 180 rows.\n",
      "\n",
      "CSV rewritten with 190 rows.\n",
      "\n",
      "CSV rewritten with 200 rows.\n",
      "\n",
      "Processing Subreddit: Austin\n",
      "CSV rewritten with 210 rows.\n",
      "\n",
      "CSV rewritten with 220 rows.\n",
      "\n",
      "CSV rewritten with 230 rows.\n",
      "\n",
      "CSV rewritten with 240 rows.\n",
      "\n",
      "CSV rewritten with 250 rows.\n",
      "\n",
      "Processing Subreddit: Portland\n",
      "CSV rewritten with 260 rows.\n",
      "\n",
      "CSV rewritten with 270 rows.\n",
      "\n",
      "CSV rewritten with 280 rows.\n",
      "\n",
      "CSV rewritten with 290 rows.\n",
      "\n",
      "CSV rewritten with 300 rows.\n",
      "\n",
      "Processing Subreddit: SanFrancisco\n",
      "CSV rewritten with 310 rows.\n",
      "\n",
      "CSV rewritten with 320 rows.\n",
      "\n",
      "CSV rewritten with 330 rows.\n",
      "\n",
      "CSV rewritten with 340 rows.\n",
      "\n",
      "CSV rewritten with 350 rows.\n",
      "\n",
      "Processing Subreddit: Boston\n",
      "CSV rewritten with 360 rows.\n",
      "\n",
      "CSV rewritten with 370 rows.\n",
      "\n",
      "CSV rewritten with 380 rows.\n",
      "\n",
      "CSV rewritten with 390 rows.\n",
      "\n",
      "CSV rewritten with 400 rows.\n",
      "\n",
      "Processing Subreddit: Houston\n",
      "CSV rewritten with 410 rows.\n",
      "\n",
      "CSV rewritten with 420 rows.\n",
      "\n",
      "CSV rewritten with 430 rows.\n",
      "\n",
      "CSV rewritten with 440 rows.\n",
      "\n",
      "CSV rewritten with 450 rows.\n",
      "\n",
      "Processing Subreddit: Atlanta\n",
      "CSV rewritten with 460 rows.\n",
      "\n",
      "CSV rewritten with 470 rows.\n",
      "\n",
      "CSV rewritten with 480 rows.\n",
      "\n",
      "CSV rewritten with 490 rows.\n",
      "\n",
      "CSV rewritten with 500 rows.\n",
      "\n",
      "Processing Subreddit: Philadelphia\n",
      "CSV rewritten with 510 rows.\n",
      "\n",
      "CSV rewritten with 520 rows.\n",
      "\n",
      "CSV rewritten with 530 rows.\n",
      "\n",
      "CSV rewritten with 540 rows.\n",
      "\n",
      "CSV rewritten with 550 rows.\n",
      "\n",
      "Processing Subreddit: Denver\n",
      "CSV rewritten with 560 rows.\n",
      "\n",
      "CSV rewritten with 570 rows.\n",
      "\n",
      "CSV rewritten with 580 rows.\n",
      "\n",
      "CSV rewritten with 590 rows.\n",
      "\n",
      "CSV rewritten with 600 rows.\n",
      "\n",
      "Processing Subreddit: SeattleWa\n",
      "CSV rewritten with 610 rows.\n",
      "\n",
      "CSV rewritten with 620 rows.\n",
      "\n",
      "CSV rewritten with 630 rows.\n",
      "\n",
      "CSV rewritten with 640 rows.\n",
      "\n",
      "CSV rewritten with 650 rows.\n",
      "\n",
      "Processing Subreddit: Dallas\n",
      "CSV rewritten with 660 rows.\n",
      "\n",
      "CSV rewritten with 670 rows.\n",
      "\n",
      "CSV rewritten with 680 rows.\n",
      "\n",
      "CSV rewritten with 690 rows.\n",
      "\n",
      "CSV rewritten with 700 rows.\n",
      "\n",
      "Processing Subreddit: WashingtonDC\n",
      "CSV rewritten with 710 rows.\n",
      "\n",
      "CSV rewritten with 720 rows.\n",
      "\n",
      "CSV rewritten with 730 rows.\n",
      "\n",
      "CSV rewritten with 731 rows.\n",
      "\n",
      "CSV rewritten with 741 rows.\n",
      "\n",
      "CSV rewritten with 750 rows.\n",
      "\n",
      "Processing Subreddit: SanDiego\n",
      "CSV rewritten with 760 rows.\n",
      "\n",
      "CSV rewritten with 770 rows.\n",
      "\n",
      "CSV rewritten with 780 rows.\n",
      "\n",
      "CSV rewritten with 790 rows.\n",
      "\n",
      "CSV rewritten with 800 rows.\n",
      "\n",
      "Processing Subreddit: Pittsburgh\n",
      "CSV rewritten with 810 rows.\n",
      "\n",
      "CSV rewritten with 820 rows.\n",
      "\n",
      "CSV rewritten with 830 rows.\n",
      "\n",
      "CSV rewritten with 840 rows.\n",
      "\n",
      "CSV rewritten with 850 rows.\n",
      "\n",
      "Processing Subreddit: Phoenix\n",
      "CSV rewritten with 860 rows.\n",
      "\n",
      "CSV rewritten with 870 rows.\n",
      "\n",
      "CSV rewritten with 880 rows.\n",
      "\n",
      "CSV rewritten with 890 rows.\n",
      "\n",
      "CSV rewritten with 900 rows.\n",
      "\n",
      "Processing Subreddit: Minneapolis\n",
      "CSV rewritten with 910 rows.\n",
      "\n",
      "CSV rewritten with 920 rows.\n",
      "\n",
      "CSV rewritten with 930 rows.\n",
      "\n",
      "CSV rewritten with 940 rows.\n",
      "\n",
      "CSV rewritten with 950 rows.\n",
      "\n",
      "Processing Subreddit: Orlando\n",
      "CSV rewritten with 960 rows.\n",
      "\n",
      "CSV rewritten with 970 rows.\n",
      "\n",
      "CSV rewritten with 980 rows.\n",
      "\n",
      "CSV rewritten with 990 rows.\n",
      "\n",
      "CSV rewritten with 1000 rows.\n",
      "\n",
      "Processing Subreddit: Nashville\n",
      "CSV rewritten with 1010 rows.\n",
      "\n",
      "CSV rewritten with 1020 rows.\n",
      "\n",
      "CSV rewritten with 1030 rows.\n",
      "\n",
      "CSV rewritten with 1040 rows.\n",
      "\n",
      "CSV rewritten with 1050 rows.\n",
      "\n",
      "Processing Subreddit: StLouis\n",
      "CSV rewritten with 1060 rows.\n",
      "\n",
      "CSV rewritten with 1070 rows.\n",
      "\n",
      "CSV rewritten with 1080 rows.\n",
      "\n",
      "CSV rewritten with 1090 rows.\n",
      "\n",
      "CSV rewritten with 1100 rows.\n",
      "\n",
      "Processing Subreddit: SaltLakeCity\n",
      "CSV rewritten with 1110 rows.\n",
      "\n",
      "CSV rewritten with 1120 rows.\n",
      "\n",
      "CSV rewritten with 1130 rows.\n",
      "\n",
      "CSV rewritten with 1140 rows.\n",
      "\n",
      "CSV rewritten with 1150 rows.\n",
      "\n",
      "Processing Subreddit: Columbus\n",
      "CSV rewritten with 1160 rows.\n",
      "\n",
      "CSV rewritten with 1170 rows.\n",
      "\n",
      "CSV rewritten with 1180 rows.\n",
      "\n",
      "CSV rewritten with 1190 rows.\n",
      "\n",
      "CSV rewritten with 1200 rows.\n",
      "\n",
      "Processing Subreddit: Raleigh\n",
      "CSV rewritten with 1210 rows.\n",
      "\n",
      "CSV rewritten with 1220 rows.\n",
      "\n",
      "CSV rewritten with 1230 rows.\n",
      "\n",
      "CSV rewritten with 1240 rows.\n",
      "\n",
      "CSV rewritten with 1250 rows.\n",
      "\n",
      "Completed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>submission_id</th>\n",
       "      <th>score</th>\n",
       "      <th>comment_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>k2q1hf9</th>\n",
       "      <td>NYC</td>\n",
       "      <td>16vc42t</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I certainly feel empathy for those with baseme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k2qbken</th>\n",
       "      <td>NYC</td>\n",
       "      <td>16vc42t</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RIP people moving this weekend, since it's the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k2q9tir</th>\n",
       "      <td>NYC</td>\n",
       "      <td>16vc42t</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Damn. That’s someone’s $3000 studio basement  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k2q7a36</th>\n",
       "      <td>NYC</td>\n",
       "      <td>16vc42t</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Empire Blvd and Flatbush was flooded and that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k2q9qyc</th>\n",
       "      <td>NYC</td>\n",
       "      <td>16vc42t</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The subways are definitely flooding today.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k46yme1</th>\n",
       "      <td>Raleigh</td>\n",
       "      <td>1740lia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Went there for a Disney on Ice and searched fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k46hq3e</th>\n",
       "      <td>Raleigh</td>\n",
       "      <td>1740lia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Don't forget, taxpayers paid for that building...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k46lbcq</th>\n",
       "      <td>Raleigh</td>\n",
       "      <td>1740lia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>You can walk up to the 3rd floor without needi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k47lq5r</th>\n",
       "      <td>Raleigh</td>\n",
       "      <td>1740lia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.00 water is a crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k46etod</th>\n",
       "      <td>Raleigh</td>\n",
       "      <td>1740lia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>That’s messed up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1250 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        subreddit submission_id score  \\\n",
       "k2q1hf9       NYC       16vc42t   NaN   \n",
       "k2qbken       NYC       16vc42t   NaN   \n",
       "k2q9tir       NYC       16vc42t   NaN   \n",
       "k2q7a36       NYC       16vc42t   NaN   \n",
       "k2q9qyc       NYC       16vc42t   NaN   \n",
       "...           ...           ...   ...   \n",
       "k46yme1   Raleigh       1740lia   NaN   \n",
       "k46hq3e   Raleigh       1740lia   NaN   \n",
       "k46lbcq   Raleigh       1740lia   NaN   \n",
       "k47lq5r   Raleigh       1740lia   NaN   \n",
       "k46etod   Raleigh       1740lia   NaN   \n",
       "\n",
       "                                              comment_body  \n",
       "k2q1hf9  I certainly feel empathy for those with baseme...  \n",
       "k2qbken  RIP people moving this weekend, since it's the...  \n",
       "k2q9tir  Damn. That’s someone’s $3000 studio basement  ...  \n",
       "k2q7a36  Empire Blvd and Flatbush was flooded and that ...  \n",
       "k2q9qyc         The subways are definitely flooding today.  \n",
       "...                                                    ...  \n",
       "k46yme1  Went there for a Disney on Ice and searched fo...  \n",
       "k46hq3e  Don't forget, taxpayers paid for that building...  \n",
       "k46lbcq  You can walk up to the 3rd floor without needi...  \n",
       "k47lq5r                              6.00 water is a crime  \n",
       "k46etod                                   That’s messed up  \n",
       "\n",
       "[1250 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_SUBREDDITS = len(subreddits)\n",
    "TARGET_COMMENTS_PER_SUBREDDIT = 50\n",
    "MAX_COMMENTS_PER_SUBMISSION = 10\n",
    "MAX_COMMENT_LENGTH = 2000\n",
    "\n",
    "collect_comments(\n",
    "    filename=filename,\n",
    "    target_comments_per_subreddit=TARGET_COMMENTS_PER_SUBREDDIT,\n",
    "    max_comments_per_submission=MAX_COMMENTS_PER_SUBMISSION,\n",
    "    max_comment_length=MAX_COMMENT_LENGTH,\n",
    "    reddit=reddit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>submission_id</th>\n",
       "      <th>score</th>\n",
       "      <th>comment_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>k2q1hf9</td>\n",
       "      <td>NYC</td>\n",
       "      <td>16vc42t</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I certainly feel empathy for those with baseme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>k2qbken</td>\n",
       "      <td>NYC</td>\n",
       "      <td>16vc42t</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RIP people moving this weekend, since it's the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>k2q9tir</td>\n",
       "      <td>NYC</td>\n",
       "      <td>16vc42t</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Damn. That’s someone’s $3000 studio basement  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>k2q7a36</td>\n",
       "      <td>NYC</td>\n",
       "      <td>16vc42t</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Empire Blvd and Flatbush was flooded and that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>k2q9qyc</td>\n",
       "      <td>NYC</td>\n",
       "      <td>16vc42t</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The subways are definitely flooding today.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>k46yme1</td>\n",
       "      <td>Raleigh</td>\n",
       "      <td>1740lia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Went there for a Disney on Ice and searched fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>k46hq3e</td>\n",
       "      <td>Raleigh</td>\n",
       "      <td>1740lia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Don't forget, taxpayers paid for that building...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>k46lbcq</td>\n",
       "      <td>Raleigh</td>\n",
       "      <td>1740lia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>You can walk up to the 3rd floor without needi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>k47lq5r</td>\n",
       "      <td>Raleigh</td>\n",
       "      <td>1740lia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.00 water is a crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>k46etod</td>\n",
       "      <td>Raleigh</td>\n",
       "      <td>1740lia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>That’s messed up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1250 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id subreddit submission_id  score  \\\n",
       "0     k2q1hf9       NYC       16vc42t    NaN   \n",
       "1     k2qbken       NYC       16vc42t    NaN   \n",
       "2     k2q9tir       NYC       16vc42t    NaN   \n",
       "3     k2q7a36       NYC       16vc42t    NaN   \n",
       "4     k2q9qyc       NYC       16vc42t    NaN   \n",
       "...       ...       ...           ...    ...   \n",
       "1245  k46yme1   Raleigh       1740lia    NaN   \n",
       "1246  k46hq3e   Raleigh       1740lia    NaN   \n",
       "1247  k46lbcq   Raleigh       1740lia    NaN   \n",
       "1248  k47lq5r   Raleigh       1740lia    NaN   \n",
       "1249  k46etod   Raleigh       1740lia    NaN   \n",
       "\n",
       "                                           comment_body  \n",
       "0     I certainly feel empathy for those with baseme...  \n",
       "1     RIP people moving this weekend, since it's the...  \n",
       "2     Damn. That’s someone’s $3000 studio basement  ...  \n",
       "3     Empire Blvd and Flatbush was flooded and that ...  \n",
       "4            The subways are definitely flooding today.  \n",
       "...                                                 ...  \n",
       "1245  Went there for a Disney on Ice and searched fo...  \n",
       "1246  Don't forget, taxpayers paid for that building...  \n",
       "1247  You can walk up to the 3rd floor without needi...  \n",
       "1248                              6.00 water is a crime  \n",
       "1249                                   That’s messed up  \n",
       "\n",
       "[1250 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
